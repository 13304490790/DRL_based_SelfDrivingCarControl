{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unity ML Agents\n",
    "## Environment Basics\n",
    "This notebook contains a walkthrough of the basic functions of the Python API for Unity ML Agents. For instructions on building a Unity environment, see [here](https://github.com/Unity-Technologies/ml-agents/wiki/Getting-Started-with-Balance-Ball)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Set environment parameters\n",
    "\n",
    "Be sure to set `env_name` to the name of the Unity environment file you want to launch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_name = \"./environment/jeju_camp\" # Name of the Unity environment binary to launch\n",
    "train_mode = True # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Examine environment parameters\n",
    "print(str(env))\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the observation and state spaces\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, _states_ refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, _observations_ refer to a set of relevant pixel-wise visuals for an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "Num_obs = len(env_info.visual_observations)\n",
    "\n",
    "print(\"Agent observations look like:\")\n",
    "if Num_obs > 1:\n",
    "    f, axarr = plt.subplots(1, Num_obs, figsize=(20,10))\n",
    "    for i, observation in enumerate(env_info.visual_observations):\n",
    "        if observation.shape[3] == 3:\n",
    "            axarr[i].imshow(observation[0,:,:,:])\n",
    "            axarr[i].axis('off')\n",
    "        else:\n",
    "            axarr[i].imshow(observation[0,:,:,0])\n",
    "            axarr[i].axis('off')\n",
    "else:\n",
    "    f, axarr = plt.subplots(1, Num_obs)\n",
    "    for i, observation in enumerate(env_info.visual_observations):\n",
    "        if observation.shape[3] == 3:\n",
    "            axarr.imshow(observation[0,:,:,:])\n",
    "            axarr.axis('off')\n",
    "        else:\n",
    "            axarr.imshow(observation[0,:,:,0])\n",
    "            axarr.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "Gamma = 0.99\n",
    "Learning_rate = 0.00025\n",
    "\n",
    "algorithm = 'DuelingDQN'\n",
    "Num_action = brain.vector_action_space_size\n",
    "\n",
    "Num_replay_memory = 50000\n",
    "Num_start_training = 25000\n",
    "Num_training = 500000\n",
    "Num_update = 5000\n",
    "Num_batch = 32\n",
    "Num_test = 100000\n",
    "Num_skipFrame = 4\n",
    "Num_stackFrame = 4\n",
    "Num_colorChannel = 1\n",
    "\n",
    "Epsilon = 1.0\n",
    "Final_epsilon = 0.1\n",
    "\n",
    "Num_plot_episode = 20\n",
    "Num_step_save = 50000\n",
    "\n",
    "GPU_fraction = 0.5\n",
    "\n",
    "img_size = 80\n",
    "\n",
    "first_conv   = [8,8,Num_colorChannel * Num_stackFrame * Num_obs,32]\n",
    "second_conv  = [4,4,32,64]\n",
    "third_conv   = [3,3,64,64]\n",
    "first_dense  = [10*10*64, 512]\n",
    "second_dense_state  = [first_dense[1], 1]\n",
    "second_dense_action = [first_dense[1], Num_action]\n",
    "\n",
    "train_mode = True\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "score_list = []\n",
    "check_show_progress = 0\n",
    "\n",
    "load_path = 'saved_networks/2018-04-20_0_32_DuelingDQN/model.ckpt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize weights and bias\n",
    "def weight_variable(shape):\n",
    "    return tf.Variable(xavier_initializer(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "\treturn tf.Variable(xavier_initializer(shape))\n",
    "\n",
    "def mu_variable(shape):\n",
    "    return tf.Variable(tf.random_uniform(shape, minval = -tf.sqrt(3/shape[0]), maxval = tf.sqrt(3/shape[0])))\n",
    "\n",
    "def sigma_variable(shape):\n",
    "\treturn tf.Variable(tf.constant(0.017, shape = shape))\n",
    "\n",
    "# Xavier Weights initializer\n",
    "def xavier_initializer(shape):\n",
    "\tdim_sum = np.sum(shape)\n",
    "\tif len(shape) == 1:\n",
    "\t\tdim_sum += 1\n",
    "\tbound = np.sqrt(2.0 / dim_sum)\n",
    "\treturn tf.random_uniform(shape, minval=-bound, maxval=bound)\n",
    "\n",
    "# Convolution and pooling\n",
    "def conv2d(x,w, stride):\n",
    "\treturn tf.nn.conv2d(x,w,strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "\treturn tf.nn.max_pool(x, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "def assign_network_to_target():\n",
    "\t# Get trainable variables\n",
    "\ttrainable_variables = tf.trainable_variables()\n",
    "\t# network lstm variables\n",
    "\ttrainable_variables_network = [var for var in trainable_variables if var.name.startswith('network')]\n",
    "\n",
    "\t# target lstm variables\n",
    "\ttrainable_variables_target = [var for var in trainable_variables if var.name.startswith('target')]\n",
    "\n",
    "\tfor i in range(len(trainable_variables_network)):\n",
    "\t\tsess.run(tf.assign(trainable_variables_target[i], trainable_variables_network[i]))\n",
    "\n",
    "# Code for tensorboard\n",
    "def setup_summary():\n",
    "    episode_speed      = tf.Variable(0.)\n",
    "    episode_overtake   = tf.Variable(0.)\n",
    "    episode_lanechange = tf.Variable(0.)\n",
    "\n",
    "    tf.summary.scalar('Average_Speed/' + str(Num_plot_episode) + 'episodes', episode_speed)\n",
    "    tf.summary.scalar('Average_overtake/' + str(Num_plot_episode) + 'episodes', episode_overtake)\n",
    "    tf.summary.scalar('Average_lanechange/' + str(Num_plot_episode) + 'episodes', episode_lanechange)\n",
    "\n",
    "    summary_vars = [episode_speed, episode_overtake, episode_lanechange]\n",
    "    summary_placeholders = [tf.placeholder(tf.float32) for _ in range(len(summary_vars))]\n",
    "    update_ops = [summary_vars[i].assign(summary_placeholders[i]) for i in range(len(summary_vars))]\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    return summary_placeholders, update_ops, summary_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input\n",
    "x_image = tf.placeholder(tf.float32, shape = [None, img_size, img_size, Num_colorChannel * Num_stackFrame * Num_obs])\n",
    "x_normalize = (x_image - (255.0/2)) / (255.0/2)\n",
    "\n",
    "with tf.variable_scope('network'):\n",
    "\t# Convolution variables\n",
    "\tw_conv1 = weight_variable(first_conv)\n",
    "\tb_conv1 = bias_variable([first_conv[3]])\n",
    "\n",
    "\tw_conv2 = weight_variable(second_conv)\n",
    "\tb_conv2 = bias_variable([second_conv[3]])\n",
    "\n",
    "\tw_conv3 = weight_variable(third_conv)\n",
    "\tb_conv3 = bias_variable([third_conv[3]])\n",
    "\n",
    "\t# Densely connect layer variables\n",
    "\tw_fc1_1 = weight_variable(first_dense)\n",
    "\tb_fc1_1 = bias_variable([first_dense[1]])\n",
    "\n",
    "\tw_fc1_2 = weight_variable(first_dense)\n",
    "\tb_fc1_2 = bias_variable([first_dense[1]])\n",
    "\n",
    "\tw_fc2_1 = weight_variable(second_dense_state)\n",
    "\tb_fc2_1 = bias_variable([second_dense_state[1]])\n",
    "\n",
    "\tw_fc2_2 = weight_variable(second_dense_action)\n",
    "\tb_fc2_2 = bias_variable([second_dense_action[1]])\n",
    "\n",
    "# Network\n",
    "h_conv1 = tf.nn.relu(conv2d(x_normalize, w_conv1, 4) + b_conv1)\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1, w_conv2, 2) + b_conv2)\n",
    "h_conv3 = tf.nn.relu(conv2d(h_conv2, w_conv3, 1) + b_conv3)\n",
    "\n",
    "h_pool3_flat = tf.reshape(h_conv3, [-1, first_dense[0]])\n",
    "h_fc1_state  = tf.nn.relu(tf.matmul(h_pool3_flat, w_fc1_1)+b_fc1_1)\n",
    "h_fc1_action = tf.nn.relu(tf.matmul(h_pool3_flat, w_fc1_2)+b_fc1_2)\n",
    "\n",
    "h_fc2_state  = tf.matmul(h_fc1_state,  w_fc2_1)+b_fc2_1\n",
    "h_fc2_action = tf.matmul(h_fc1_action, w_fc2_2)+b_fc2_2\n",
    "\n",
    "h_fc2_advantage = tf.subtract(h_fc2_action, tf.reduce_mean(h_fc2_action))\n",
    "\n",
    "output = tf.add(h_fc2_state, h_fc2_advantage)\n",
    "\n",
    "with tf.variable_scope('target'):\n",
    "\t# Convolution variables target\n",
    "\tw_conv1_target = weight_variable(first_conv)\n",
    "\tb_conv1_target = bias_variable([first_conv[3]])\n",
    "\n",
    "\tw_conv2_target = weight_variable(second_conv)\n",
    "\tb_conv2_target = bias_variable([second_conv[3]])\n",
    "\n",
    "\tw_conv3_target = weight_variable(third_conv)\n",
    "\tb_conv3_target = bias_variable([third_conv[3]])\n",
    "\n",
    "\t# Densely connect layer variables target\n",
    "\tw_fc1_1_target = weight_variable(first_dense)\n",
    "\tb_fc1_1_target = bias_variable([first_dense[1]])\n",
    "\n",
    "\tw_fc1_2_target = weight_variable(first_dense)\n",
    "\tb_fc1_2_target = bias_variable([first_dense[1]])\n",
    "\n",
    "\tw_fc2_1_target = weight_variable(second_dense_state)\n",
    "\tb_fc2_1_target = bias_variable([second_dense_state[1]])\n",
    "\n",
    "\tw_fc2_2_target = weight_variable(second_dense_action)\n",
    "\tb_fc2_2_target = bias_variable([second_dense_action[1]])\n",
    "\n",
    "# Target Network\n",
    "h_conv1_target = tf.nn.relu(conv2d(x_normalize, w_conv1_target, 4) + b_conv1_target)\n",
    "h_conv2_target = tf.nn.relu(conv2d(h_conv1_target, w_conv2_target, 2) + b_conv2_target)\n",
    "h_conv3_target = tf.nn.relu(conv2d(h_conv2_target, w_conv3_target, 1) + b_conv3_target)\n",
    "\n",
    "h_pool3_flat_target = tf.reshape(h_conv3_target, [-1, first_dense[0]])\n",
    "\n",
    "h_fc1_state_target  = tf.nn.relu(tf.matmul(h_pool3_flat_target, w_fc1_1_target)+b_fc1_1_target)\n",
    "h_fc1_action_target = tf.nn.relu(tf.matmul(h_pool3_flat_target, w_fc1_2_target)+b_fc1_2_target)\n",
    "\n",
    "h_fc2_state_target  = tf.matmul(h_fc1_state_target,  w_fc2_1_target)+b_fc2_1_target\n",
    "h_fc2_action_target = tf.matmul(h_fc1_action_target, w_fc2_2_target)+b_fc2_2_target\n",
    "\n",
    "h_fc2_advantage_target = tf.subtract(h_fc2_action_target, tf.reduce_mean(h_fc2_action_target))\n",
    "\n",
    "output_target = tf.add(h_fc2_state_target, h_fc2_advantage_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loss function and Train\n",
    "action_target = tf.placeholder(tf.float32, shape = [None, Num_action])\n",
    "y_target = tf.placeholder(tf.float32, shape = [None])\n",
    "\n",
    "y_prediction = tf.reduce_sum(tf.multiply(output, action_target), reduction_indices = 1)\n",
    "Loss = tf.reduce_mean(tf.square(y_prediction - y_target))\n",
    "train_step = tf.train.AdamOptimizer(learning_rate = Learning_rate, epsilon = 1e-02).minimize(Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Initialize variables\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = GPU_fraction\n",
    "\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training or Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the file if the saved file exists\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# check_save = 1\n",
    "check_save = input('Inference? / Training?(1=Inference/2=Training): ')\n",
    "\n",
    "if check_save == '1':\n",
    "    # Directly start inference\n",
    "    Num_start_training = 0\n",
    "    Num_training = 0\n",
    "    \n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, load_path)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "# date - hour - minute of training time\n",
    "date_time = str(datetime.date.today()) + '_' + str(datetime.datetime.now().hour) + '_' + str(datetime.datetime.now().minute)\n",
    "\n",
    "# Make folder for save data\n",
    "os.makedirs('saved_networks/' + date_time + '_' + algorithm)\n",
    "\n",
    "# Summary for tensorboard\n",
    "summary_placeholders, update_ops, summary_op = setup_summary()\n",
    "summary_writer = tf.summary.FileWriter('saved_networks/' + date_time + '_' + algorithm, sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def input_initialization(env_info):\n",
    "    observation_stack_obs = np.zeros([img_size, img_size, Num_colorChannel * Num_obs])\n",
    "    \n",
    "    for i in range(Num_obs):\n",
    "        observation = 255 * env_info.visual_observations[i]\n",
    "        observation = np.uint8(observation)\n",
    "        observation = np.reshape(observation, (observation.shape[1], observation.shape[2], 3))\n",
    "        observation = cv2.resize(observation, (img_size, img_size))\n",
    "\n",
    "        if Num_colorChannel == 1:\n",
    "            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "            observation = np.reshape(observation, (img_size, img_size))\n",
    "\n",
    "        if Num_colorChannel == 3:\n",
    "            observation_stack_obs[:,:, Num_colorChannel * i: Num_colorChannel * (i+1)] = observation\n",
    "        else:\n",
    "            observation_stack_obs[:,:, i] = observation\n",
    "\n",
    "    observation_set = []\n",
    "\n",
    "    for i in range(Num_skipFrame * Num_stackFrame):\n",
    "        observation_set.append(observation_stack_obs)\n",
    "    \n",
    "    # Stack the frame according to the number of skipping and stacking frames using observation set\n",
    "    observation_stack = np.zeros((img_size, img_size, Num_colorChannel * Num_stackFrame * Num_obs))\n",
    "    for stack_frame in range(Num_stackFrame):\n",
    "        observation_stack[:,:,Num_obs * stack_frame: Num_obs * (stack_frame+1)] = observation_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "\n",
    "    observation_stack = np.uint8(observation_stack)\n",
    "\n",
    "    return observation_stack, observation_set\n",
    "\n",
    "def resize_input(env_info, observation_set):\n",
    "    # Stack observation according to the number of observations\n",
    "    observation_stack_obs = np.zeros([img_size, img_size, Num_colorChannel * Num_obs])\n",
    "\n",
    "    for i in range(Num_obs):\n",
    "        observation = 255 * env_info.visual_observations[i]\n",
    "        observation = np.uint8(observation)\n",
    "        observation = np.reshape(observation, (observation.shape[1], observation.shape[2], 3))\n",
    "        observation = cv2.resize(observation, (img_size, img_size))\n",
    "        \n",
    "        if Num_colorChannel == 1:\n",
    "            observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "            observation = np.reshape(observation, (img_size, img_size))\n",
    "\n",
    "        if Num_colorChannel == 3:\n",
    "            observation_stack_obs[:,:, Num_colorChannel * i: Num_colorChannel * (i+1)] = observation\n",
    "        else:\n",
    "            observation_stack_obs[:,:,i] = observation\n",
    "    \n",
    "    # Add observations to the observation_set\n",
    "    observation_set.append(observation_stack_obs)\n",
    "    \n",
    "    # Stack the frame according to the number of skipping and stacking frames using observation set\n",
    "    observation_stack = np.zeros((img_size, img_size, Num_colorChannel * Num_stackFrame * Num_obs))\n",
    "    for stack_frame in range(Num_stackFrame):\n",
    "        observation_stack[:,:,Num_obs * stack_frame: Num_obs * (stack_frame+1)] = observation_set[-1 - (Num_skipFrame * stack_frame)]\n",
    "\n",
    "    del observation_set[0]\n",
    "\n",
    "    observation_stack = np.uint8(observation_stack)\n",
    "    \n",
    "    return observation_stack, observation_set\n",
    "\n",
    "# Get progress according to the \n",
    "def get_progress(step, Epsilon):\n",
    "    if step <= Num_start_training:\n",
    "        # Observation\n",
    "        progress = 'Observing'\n",
    "        train_mode = True\n",
    "        Epsilon = 1\n",
    "\n",
    "    elif step <= Num_start_training + Num_training:\n",
    "        # Training\n",
    "        progress = 'Training'\n",
    "        train_mode = True\n",
    "        \n",
    "        # Decrease the epsilon value\n",
    "        if Epsilon > Final_epsilon:\n",
    "            Epsilon -= 1.0/Num_training\n",
    "        \n",
    "    elif step < Num_start_training + Num_training + Num_test:\n",
    "        # Testing\n",
    "        progress = 'Testing'\n",
    "        train_mode = False\n",
    "        Epsilon = 0\n",
    "\n",
    "    else:\n",
    "        # Finished\n",
    "        progress = 'Finished'\n",
    "        train_mode = False\n",
    "        Epsilon = 0\n",
    "        \n",
    "    return progress, train_mode, Epsilon \n",
    "\n",
    "# Select action according to the progress of training\n",
    "def select_action(progress, sess, observation_stack, Epsilon):\n",
    "    if progress == \"Observing\":\n",
    "        Q_value = 0\n",
    "        action = np.zeros([Num_action])\n",
    "        action[random.randint(0, Num_action - 1)] = 1.0\n",
    "    elif progress == \"Training\":\n",
    "        # if random value(0 - 1) is smaller than Epsilon, action is random. Otherwise, action is the one which has the largest Q value\n",
    "        if random.random() < Epsilon:\n",
    "            Q_value = 0\n",
    "            action = np.zeros([Num_action])\n",
    "            action[random.randint(0, Num_action - 1)] = 1\n",
    "        else:\n",
    "            Q_value = output.eval(feed_dict={x_image: [observation_stack]})\n",
    "            action = np.zeros([Num_action])\n",
    "            action[np.argmax(Q_value)] = 1\n",
    "    else:\n",
    "        Q_value = output.eval(feed_dict={x_image: [observation_stack]})\n",
    "        action = np.zeros([Num_action])\n",
    "        action[np.argmax(Q_value)] = 1\n",
    "        \n",
    "    return action, Q_value\n",
    "\n",
    "def train(Replay_memory, sess, step):\n",
    "    # Select minibatch\n",
    "    minibatch =  random.sample(Replay_memory, Num_batch)\n",
    "\n",
    "    # Save the each batch data\n",
    "    observation_batch      = [batch[0] for batch in minibatch]\n",
    "    action_batch           = [batch[1] for batch in minibatch]\n",
    "    reward_batch           = [batch[2] for batch in minibatch]\n",
    "    observation_next_batch = [batch[3] for batch in minibatch]\n",
    "    terminal_batch \t       = [batch[4] for batch in minibatch]\n",
    "\n",
    "    # Update target network according to the Num_update value\n",
    "    if step % Num_update == 0:\n",
    "        assign_network_to_target()\n",
    "\n",
    "    ########################################### Noisy Network ###########################################\n",
    "    # Get y_prediction\n",
    "    y_batch = []\n",
    "    Q_batch = output_target.eval(feed_dict = {x_image: observation_next_batch})\n",
    "    for i in range(len(minibatch)):\n",
    "        if terminal_batch[i] == True:\n",
    "            y_batch.append(reward_batch[i])\n",
    "        else:\n",
    "            y_batch.append(reward_batch[i] + Gamma * np.max(Q_batch[i]))\n",
    "\n",
    "    _, loss = sess.run([train_step, Loss], feed_dict = {action_target: action_batch, y_target: y_batch, x_image: observation_batch})\n",
    "    #####################################################################################################\n",
    "\n",
    "# Experience Replay \n",
    "def Experience_Replay(progress, Replay_memory, observation_stack, action, reward, next_observation_stack, terminal):\n",
    "    if progress != 'Testing':\n",
    "        # If length of replay memeory is more than the setting value then remove the first one\n",
    "        if len(Replay_memory) > Num_replay_memory:\n",
    "            del Replay_memory[0]\n",
    "\n",
    "        # Save experience to the Replay memory\n",
    "        Replay_memory.append([observation_stack, action, reward, next_observation_stack, terminal])\n",
    "    else:\n",
    "        # Empty the replay memory if testing\n",
    "        Replay_memory = []\n",
    "    \n",
    "    return Replay_memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initial parameters\n",
    "Replay_memory = []\n",
    "\n",
    "step = 1\n",
    "score = 0\n",
    "score_board = 0\n",
    "\n",
    "episode = 0\n",
    "step_per_episode = 0\n",
    "\n",
    "speed_list = []\n",
    "overtake_list = []\n",
    "lanechange_list = []\n",
    "\n",
    "train_mode = True\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "observation_stack, observation_set = input_initialization(env_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "check_plot = 0\n",
    "\n",
    "# Training & Testing\n",
    "while True:\n",
    "    \n",
    "    # Get Progress, train mode\n",
    "    progress, train_mode, Epsilon  = get_progress(step, Epsilon)\n",
    "    \n",
    "    # Select Actions \n",
    "    action, Q_value = select_action(progress, sess, observation_stack, Epsilon)\n",
    "    action_in = [np.argmax(action)]\n",
    "\n",
    "    # Get information for plotting\n",
    "    vehicle_speed  = 100 * env_info.vector_observations[0][-3]\n",
    "    num_overtake   = env_info.vector_observations[0][-2]\n",
    "    num_lanechange = env_info.vector_observations[0][-1]\n",
    "    \n",
    "    # Get information for update\n",
    "    env_info = env.step(action_in)[default_brain]\n",
    "\n",
    "    next_observation_stack, observation_set = resize_input(env_info, observation_set) \n",
    "    reward = env_info.rewards[0]\n",
    "    terminal = env_info.local_done[0]\n",
    "    \n",
    "    if progress == 'Training':\n",
    "        # Train!! \n",
    "        train(Replay_memory, sess, step)\n",
    "\n",
    "        # Save the variables to disk.\n",
    "        if step == Num_start_training + Num_training:\n",
    "            save_path = saver.save(sess, 'saved_networks/' + date_time + '_' + algorithm + \"/model.ckpt\")\n",
    "            print(\"Model saved in file: %s\" % save_path)\n",
    "    \n",
    "    # If progress is finished -> close! \n",
    "    if progress == 'Finished':\n",
    "        print('Finished!!')\n",
    "        env.close()\n",
    "        break\n",
    "        \n",
    "    Replay_memory = Experience_Replay(progress, Replay_memory, observation_stack, action, reward, next_observation_stack, terminal)\n",
    "    \n",
    "    # Update information\n",
    "    step += 1\n",
    "    score += reward\n",
    "    step_per_episode += 1\n",
    "    observation_stack = next_observation_stack\n",
    "    \n",
    "    # Update tensorboard\n",
    "    if progress != 'Observing':\n",
    "        speed_list.append(vehicle_speed)\n",
    "        \n",
    "        if episode % Num_plot_episode == 0 and check_plot == 1 and episode != 0:\n",
    "            avg_speed      = sum(speed_list) / len(speed_list)\n",
    "            avg_overtake   = sum(overtake_list) / len(overtake_list)\n",
    "            avg_lanechange = sum(lanechange_list) / len(lanechange_list)\n",
    "            \n",
    "            tensorboard_info = [avg_speed, avg_overtake, avg_lanechange]\n",
    "            for i in range(len(tensorboard_info)):\n",
    "                sess.run(update_ops[i], feed_dict = {summary_placeholders[i]: float(tensorboard_info[i])})\n",
    "            summary_str = sess.run(summary_op)\n",
    "            summary_writer.add_summary(summary_str, step)\n",
    "            score_board = 0\n",
    "            \n",
    "            speed_list = []\n",
    "            overtake_list = []\n",
    "            lanechange_list = []\n",
    "\n",
    "            check_plot = 0\n",
    "            \n",
    "    # If terminal is True\n",
    "    if terminal == True:\n",
    "        # Print informations\n",
    "        print('step: ' + str(step) + ' / '  + 'episode: ' + str(episode) + ' / ' + 'progress: ' + progress  + ' / ' + 'epsilon: ' + str(Epsilon)  +' / ' + 'score: ' + str(score))\n",
    "\n",
    "        check_plot = 1\n",
    "\n",
    "        if progress != 'Observing':\n",
    "            episode += 1\n",
    "            \n",
    "            score_board += score\n",
    "            overtake_list.append(num_overtake)\n",
    "            lanechange_list.append(num_lanechange)\n",
    "            \n",
    "        score = 0\n",
    "        step_per_episode = 0\n",
    "\n",
    "        # Initialize game state\n",
    "        env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "        observation_stack, observation_set = input_initialization(env_info)\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
